{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-LohleBMlahL"},"outputs":[],"source":["# Copyright 2023 MIT Introduction to Deep Learning. All Rights Reserved.\n","#\n","# Licensed under the MIT License. You may not use this file except in compliance\n","# with the License. Use and/or modification of this code outside of MIT Introduction\n","# to Deep Learning must reference:\n","#\n","# Â© MIT Introduction to Deep Learning\n","# http://introtodeeplearning.com\n","#"]},{"cell_type":"markdown","metadata":{"id":"ckzz5Hus-hJB"},"source":["# Debiasing, Uncertainty, and Robustness\n","\n","# Part 1: Introduction to Capsa\n","\n","In this assignment, we'll explore different ways to make deep learning models more **robust** and **trustworthy**.\n","\n","To achieve this it is critical to be able to identify and diagnose issues of bias and uncertainty in deep learning models, as we explored in the Facial Detection assignment. We need benchmarks that uniformly measure how uncertain a given model is, and we need principled ways of measuring bias and uncertainty. To that end, in this lab, we'll utilize [Capsa](https://github.com/themis-ai/capsa), a risk-estimation wrapping library developed by [Themis AI](https://themisai.io/). Capsa supports the estimation of three different types of ***risk***, defined as measures of how robust and trustworthy our model is. These are:\n","1. **Representation bias**: reflects how likely combinations of features are to appear in a given dataset. Often, certain combinations of features are severely under-represented in datasets, which means models learn them less well and can thus lead to unwanted bias.\n","2. **Data uncertainty**: reflects noise in the data, for example when sensors have noisy measurements, classes in datasets have low separations, and generally when very similar inputs lead to drastically different outputs. Also known as *aleatoric* uncertainty.\n","3. **Model uncertainty**: captures the areas of our underlying data distribution that the model has not yet learned or has difficulty learning. Areas of high model uncertainty can be due to out-of-distribution (OOD) samples or data that is harder to learn. Also known as *epistemic* uncertainty."]},{"cell_type":"markdown","metadata":{"id":"o02MyoDrnNqP"},"source":["## CAPSA overview\n","\n","This assignment introduces Capsa and its functionalities, to next build automated tools that use Capsa to mitigate the underlying issues of bias and uncertainty.\n","\n","The core idea behind [Capsa](https://themisai.io/capsa/) is that any deep learning model of interest can be ***wrapped*** -- just like wrapping a gift -- to be made ***aware of its own risks***. Risk is captured in representation bias, data uncertainty, and model uncertainty.\n","\n","![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/2023/lab3/img/capsa_overview.png)\n","\n","This means that Capsa takes the user's original model as input, and modifies it minimally to create a risk-aware variant while preserving the model's underlying structure and training pipeline. Capsa is a one-line addition to any training workflow in TensorFlow. In this part of the lab, we'll apply Capsa's risk estimation methods to a simple regression problem to further explore the notions of bias and uncertainty.\n","\n","Please refer to [Capsa's documentation](https://themisai.io/capsa/) for additional details."]},{"cell_type":"markdown","metadata":{"id":"hF0uSqk-nwmA"},"source":["Let's get started by installing the necessary dependencies:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NdXF4Reyj6yy"},"outputs":[],"source":["# Import Tensorflow 2.0\n","#%tensorflow_version 2.x\n","import tensorflow as tf\n","\n","import IPython\n","import functools\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm\n","\n","# Download and import the MIT 6.S191 package\n","!pip install git+https://github.com/aamini/introtodeeplearning.git@2023\n","import mitdeeplearning as mdl\n","\n","# Download and import capsa\n","!pip install capsa\n","import capsa"]},{"cell_type":"markdown","metadata":{"id":"xzEcxjKHn8gc"},"source":["## 1.1 Dataset\n","\n","We will build understanding of bias and uncertainty by training a neural network for a simple 2D regression task: modeling the function $y = x^3$. We will use Capsa to analyze this dataset and the performance of the model. Noise and missing-ness will be injected into the dataset.\n","\n","Let's generate the dataset and visualize it:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fH40EhC1j9dH"},"outputs":[],"source":["# Get the data for the cubic function, injected with noise and missing-ness\n","# This is just a toy dataset that we can use to test some of the wrappers on\n","def gen_data(x_min, x_max, n, train=True):\n","  if train:\n","    x = np.random.triangular(x_min, 2, x_max, size=(n, 1))\n","  else:\n","    x = np.linspace(x_min, x_max, n).reshape(n, 1)\n","\n","  sigma = 2*np.exp(-(x+1)**2/1) + 0.2 if train else np.zeros_like(x)\n","  y = x**3/6 + np.random.normal(0, sigma).astype(np.float32)\n","\n","  return x, y\n","\n","# Plot the dataset and visualize the train and test datapoints\n","x_train, y_train = gen_data(-4, 4, 2000, train=True) # train data\n","x_test, y_test = gen_data(-6, 6, 500, train=False) # test data\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(x_test, y_test, c='r', zorder=-1, label='ground truth')\n","plt.scatter(x_train, y_train, s=1.5, label='train data')\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"Fz3UxT8vuN95"},"source":["In the plot above, the blue points are the training data, which will be used as inputs to train the neural network model. The red line is the ground truth data, which will be used to evaluate the performance of the model.\n","\n","#### **Inspecting the 2D regression dataset**\n","\n"," Think about the answers to the questions below to improve your understanding of the topic:\n","\n","1. What are your observations about where the train data and test data lie relative to each other?\n","2. What, if any, areas do you expect to have high/low aleatoric (data) uncertainty?\n","3. What, if any, areas do you expect to have high/low epistemic (model) uncertainty?"]},{"cell_type":"markdown","metadata":{"id":"mXMOYRHnv8tF"},"source":["## 1.2 Regression on cubic dataset\n","\n","Next we will define a small dense neural network model that can predict `y` given `x`: this is a classical regression task! We will build the model and use the [`model.fit()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) function to train the model -- normally, without any risk-awareness -- using the train dataset that we visualized above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7p1XwfZVuB68"},"outputs":[],"source":["### Define and train a dense NN model for the regression task###\n","\n","'''Function to define a small dense NN'''\n","def create_dense_NN():\n","  return tf.keras.Sequential(\n","          [\n","              tf.keras.Input(shape=(1,)),\n","              tf.keras.layers.Dense(32, \"relu\"),\n","              tf.keras.layers.Dense(32, \"relu\"),\n","              tf.keras.layers.Dense(32, \"relu\"),\n","              tf.keras.layers.Dense(1),\n","          ]\n","  )\n","\n","dense_NN = create_dense_NN()\n","\n","# Build the model for regression, defining the loss function and optimizer\n","dense_NN.compile(\n","  optimizer=tf.keras.optimizers.Adam(learning_rate=5e-3),\n","  loss=tf.keras.losses.MeanSquaredError(), # MSE loss for the regression task\n",")\n","\n","# Train the model for 30 epochs using model.fit().\n","loss_history = dense_NN.fit(x_train, y_train, epochs=30)"]},{"cell_type":"markdown","metadata":{"id":"ovwYBUG3wTDv"},"source":["Now, we are ready to evaluate our neural network. We use the test data to assess performance on the regression task, and visualize the predicted values against the true values.\n","\n","Given your observation of the data in the previous plot, where do you expect the model to perform well? Let's test the model and see:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fb-EklZywR4D"},"outputs":[],"source":["# Pass the test data through the network and predict the y values\n","y_predicted = dense_NN.predict(x_test)\n","\n","# Visualize the true (x, y) pairs for the test data vs. the predicted values\n","plt.figure(figsize=(10, 6))\n","plt.scatter(x_train, y_train, s=1.5, label='train data')\n","plt.plot(x_test, y_test, c='r', zorder=-1, label='ground truth')\n","plt.plot(x_test, y_predicted, c='b', zorder=0, label='predicted')\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"7Vktjwfu0ReH"},"source":["\n","#### **Analyzing the performance of standard regression model**\n","\n","Think about the answers to the questions below to improve your understanding of the topic:\n","\n","1. Where does the model perform well?\n","2. Where does the model perform poorly?"]},{"cell_type":"markdown","metadata":{"id":"7MzvM48JyZMO"},"source":["## 1.3 Evaluating bias\n","\n","Now that we've seen what the predictions from this model look like, we will identify and quantify bias and uncertainty in this problem. We first consider bias.\n","\n","Recall that *representation bias* reflects how likely combinations of features are to appear in a given dataset. Capsa calculates how likely combinations of features are by using a histogram estimation approach: the `capsa.HistogramWrapper`. For low-dimensional data, the `capsa.HistogramWrapper` bins the input directly into discrete categories and measures the density. More details of the `HistogramWrapper` and how it can be used are [available here](https://themisai.io/capsa/api_documentation/HistogramWrapper.html).\n","\n","We start by taking our `dense_NN` and wrapping it with the `capsa.HistogramWrapper`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVv-knsCwOp9"},"outputs":[],"source":["### Wrap the dense network for bias estimation ###\n","\n","standard_dense_NN = create_dense_NN()\n","bias_wrapped_dense_NN = capsa.HistogramWrapper(\n","    standard_dense_NN, # the original model\n","    num_bins=20,\n","    queue_size=2000, # how many samples to track\n","    target_hidden_layer=False # for low-dimensional data (like this dataset), we can estimate biases directly from data\n",")"]},{"cell_type":"markdown","metadata":{"id":"UFHO7LKcz8uP"},"source":["Now that we've wrapped the classifier, let's re-train it to update the bias estimates as we train. We can use the exact same training pipeline, using `compile` to build the model and `model.fit()` to train the model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkyD3rsqy2ff"},"outputs":[],"source":["### Compile and train the wrapped model! ###\n","\n","# Build the model for regression, defining the loss function and optimizer\n","bias_wrapped_dense_NN.compile(\n","  optimizer=tf.keras.optimizers.Adam(learning_rate=2e-3),\n","  loss=tf.keras.losses.MeanSquaredError(), # MSE loss for the regression task\n",")\n","\n","# Train the wrapped model for 30 epochs.\n","loss_history_bias_wrap = bias_wrapped_dense_NN.fit(x_train, y_train, epochs=30)\n","\n","print(\"Done training model with Bias Wrapper!\")"]},{"cell_type":"markdown","metadata":{"id":"_6iVeeqq0f_H"},"source":["We can now use our wrapped model to assess the bias for a given test input. With the wrapping capability, Capsa neatly allows us to output a *bias score* along with the predicted target value. This bias score reflects the density of data surrounding an input point -- the higher the score, the greater the data representation and density. The wrapped, risk-aware model outputs the predicted target and bias score after it is called!\n","\n","Let's see how it is done:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZ17eCbP0YM4"},"outputs":[],"source":["### Generate and visualize bias scores for data in test set ###\n","\n","# Call the risk-aware model to generate scores\n","predictions, bias = bias_wrapped_dense_NN(x_test)\n","\n","# Visualize the relationship between the input data x and the bias\n","fig, ax = plt.subplots(2, 1, figsize=(8,6))\n","ax[0].plot(x_test, bias, label='bias')\n","ax[0].set_ylabel('Estimated Bias')\n","ax[0].legend()\n","\n","# Let's compare against the ground truth density distribution\n","#   should roughly align with our estimated bias in this toy example\n","ax[1].hist(x_train, 50, label='ground truth')\n","ax[1].set_xlim(-6, 6)\n","ax[1].set_ylabel('True Density')\n","ax[1].legend();"]},{"cell_type":"markdown","metadata":{"id":"HpDMT_1FERQE"},"source":["#### **Evaluating bias with wrapped regression model**\n","\n","Think about the answers to the questions below to improve your understanding of the topic:\n","\n","1. How does the bias score relate to the train/test data density from the first plot?\n","2. What is one limitation of the Histogram approach that simply bins the data based on frequency?"]},{"cell_type":"markdown","metadata":{"id":"PvS8xR_q27Ec"},"source":["# 1.4 Estimating data uncertainty\n","\n","Next we turn our attention to uncertainty, first focusing on the uncertainty in the data -- the aleatoric uncertainty.\n","\n","As introduced in the lecture on Robust & Trustworthy Deep Learning, in regression we can estimate aleatoric uncertainty by training the model to predict both a target value and a variance for every input. Because we estimate both a mean and variance for every input, this method is called Mean Variance Estimation (MVE). MVE involves modifying the output layer to predict both the mean and variance, and changing the loss to reflect the prediction likelihood.\n","\n","Capsa automatically implements these changes for us: we can wrap a given model using `capsa.MVEWrapper` to use MVE to estimate aleatoric uncertainty. All we have to do is define the model and the loss function to evaluate its predictions! More details of the `MVEWrapper` and how it can be used are [available here](https://themisai.io/capsa/api_documentation/MVEWrapper.html).\n","\n","Let's take our standard network, wrap it with `capsa.MVEWrapper`, build the wrapped model, and then train it for the regression task. Finally, we evaluate performance of the resulting model by quantifying the aleatoric uncertainty across the data space:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sxmm-2sd3G9u"},"outputs":[],"source":["### Estimating data uncertainty with Capsa wrapping ###\n","\n","standard_dense_NN = create_dense_NN()\n","# Wrap the dense network for aleatoric uncertainty estimation\n","mve_wrapped_NN = capsa.MVEWrapper(standard_dense_NN)\n","\n","# Build the model for regression, defining the loss function and optimizer\n","mve_wrapped_NN.compile(\n","  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),\n","  loss=tf.keras.losses.MeanSquaredError(), # MSE loss for the regression task\n",")\n","\n","# Train the wrapped model for 30 epochs.\n","loss_history_mve_wrap = mve_wrapped_NN.fit(x_train, y_train, epochs=30)\n","\n","# Call the uncertainty-aware model to generate outputs for the test data\n","x_test_clipped = np.clip(x_test, x_train.min(), x_train.max())\n","prediction = mve_wrapped_NN(x_test_clipped)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dT2Rx8JCg3NR"},"outputs":[],"source":["# Capsa makes the aleatoric uncertainty an attribute of the prediction!\n","pred = np.array(prediction.y_hat).flatten()\n","unc = np.sqrt(prediction.aleatoric).flatten() # out.aleatoric is the predicted variance\n","\n","# Visualize the aleatoric uncertainty across the data space\n","plt.figure(figsize=(10, 6))\n","plt.scatter(x_train, y_train, s=1.5, label='train data')\n","plt.plot(x_test, y_test, c='r', zorder=-1, label='ground truth')\n","plt.fill_between(x_test_clipped.flatten(), pred-2*unc, pred+2*unc,\n","                 color='b', alpha=0.2, label='aleatoric')\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"ZFeArgRX9U9s"},"source":["#### **Estimating aleatoric uncertainty**\n","\n","Think about the answers to the questions below to improve your understanding of the topic:\n","\n","1. For what values of $x$ is the aleatoric uncertainty high or increasing suddenly?\n","2. How does your answer in (1) relate to how the $x$ values are distributed?"]},{"cell_type":"markdown","metadata":{"id":"6FC5WPRT5lAb"},"source":["# 1.5 Estimating model uncertainty\n","\n","Finally, we use Capsa for estimating the uncertainty underlying the model predictions -- the epistemic uncertainty. In this example, we'll use ensembles, which essentially copy the model `N` times and average predictions across all runs for a more robust prediction, and also calculate the variance of the `N` runs to estimate the uncertainty.\n","\n","Capsa provides a neat wrapper, `capsa.EnsembleWrapper`, to make an ensemble from an input model. Just like with aleatoric estimation, we can take our standard dense network model, wrap it with `capsa.EnsembleWrapper`, build the wrapped model, and then train it for the regression task. More details of the `EnsembleWrapper` and how it can be used are [available here](https://themisai.io/capsa/api_documentation/EnsembleWrapper.html).\n","\n","Finally, we evaluate the resulting model by quantifying the epistemic uncertainty on the test data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuRlhq2c5Fob"},"outputs":[],"source":["### Estimating model uncertainty with Capsa wrapping ###\n","\n","standard_dense_NN = create_dense_NN()\n","# Wrap the dense network for epistemic uncertainty estimation with an Ensemble\n","ensemble_NN = capsa.EnsembleWrapper(standard_dense_NN)\n","\n","# Build the model for regression, defining the loss function and optimizer\n","ensemble_NN.compile(\n","  optimizer=tf.keras.optimizers.Adam(learning_rate=3e-3),\n","  loss=tf.keras.losses.MeanSquaredError(), # MSE loss for the regression task\n",")\n","\n","# Train the wrapped model for 30 epochs.\n","loss_history_ensemble = ensemble_NN.fit(x_train, y_train, epochs=30)\n","\n","# Call the uncertainty-aware model to generate outputs for the test data\n","prediction = ensemble_NN(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eauNoKDOj_ZT"},"outputs":[],"source":["# Capsa makes the epistemic uncertainty an attribute of the prediction!\n","pred = np.array(prediction.y_hat).flatten()\n","unc = np.array(prediction.epistemic).flatten()\n","\n","# Visualize the aleatoric uncertainty across the data space\n","plt.figure(figsize=(10, 6))\n","plt.scatter(x_train, y_train, s=1.5, label='train data')\n","plt.plot(x_test, y_test, c='r', zorder=-1, label='ground truth')\n","plt.fill_between(x_test.flatten(), pred-20*unc, pred+20*unc, color='b', alpha=0.2, label='epistemic')\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"N4LMn2tLPBdg"},"source":["#### **Estimating epistemic uncertainty**\n","\n","Thibk about the answers to the following questions to test your understanding:\n","\n","1. For what values of $x$ is the epistemic uncertainty high or increasing suddenly?\n","2. How does your answer in (1) relate to how the $x$ values are distributed (refer back to original plot)? Think about both the train and test data.\n","3. How could you reduce the epistemic uncertainty in regions where it is high?"]},{"cell_type":"markdown","metadata":{"id":"CkpvkOL06jRd"},"source":["# 1.6 Conclusion\n","\n","You've just analyzed the bias, aleatoric uncertainty, and epistemic uncertainty for your first risk-aware model! This is a task that data scientists do constantly to determine methods of improving their models and datasets.\n","\n","In the next part of the assignment, you'll continue to build off of these concepts to study them in the context of facial detection systems: not only diagnosing issues of bias and uncertainty, but also developing solutions to *mitigate* these risks.\n","\n","![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/2023/lab3/img/solutions_toy.png)"]},{"cell_type":"markdown","metadata":{"id":"IgYKebt871EK"},"source":["# Part 2: Mitigating Bias and Uncertainty in Facial Detection Systems\n","\n","In a previous assignment, we defined a semi-supervised VAE (SS-VAE) to diagnose feature representation disparities and biases in facial detection systems. In part 1 of this assignment, we gained experience with [Capsa](https://github.com/themis-ai/capsa/) and its ability to build risk-aware models automatically through wrapping. Now in this lab, we will put these two together: using Capsa to build systems that can *automatically* uncover and mitigate bias and uncertainty in facial detection systems.\n","\n","As we have seen, automatically detecting and mitigating bias and uncertainty is crucial to deploying fair and safe models. Building off our foundation with Capsa, developed by [Themis AI](https://themisai.io/), we will now use Capsa for the facial detection problem, in order to diagnose risks in facial detection models. You will then design and create strategies to mitigate these risks, with goal of improving model performance across the entire facial detection dataset.\n","\n","**Your goal in this assignment is to design a strategic solution for bias and uncertainty mitigation, using Capsa.** The approaches and solutions with oustanding performance will be recognized with outstanding prizes! Details on the submission process are at the end of this lab.\n","\n","![Capsa overview](https://raw.githubusercontent.com/aamini/introtodeeplearning/2023/lab3/img/capsa_overview.png)"]},{"cell_type":"markdown","metadata":{"id":"6VKVqLb371EV"},"source":["# 2.1 Datasets\n","\n","Since we are again focusing on the facial detection problem, we will use the same datasets from that assignment. To remind you, we have a dataset of positive examples (i.e., of faces) and a dataset of negative examples (i.e., of things that are not faces).\n","\n","1.   **Positive training data**: [CelebA Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). A large-scale dataset (over 200K images) of celebrity faces.   \n","2.   **Negative training data**: [ImageNet](http://www.image-net.org/). A large-scale dataset with many images across many different categories. We will take negative examples from a variety of non-human categories.\n","\n","We will evaluate trained models on an independent test dataset of face images to diagnose and mitigate potential issues with *bias, fairness, and confidence*. This will be a larger test dataset for evaluation purposes.\n","\n","We begin by importing these datasets. We have defined a `DatasetLoader` class that does a bit of data pre-processing to import the training data in a usable format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HIA6EA1D71EW"},"outputs":[],"source":["batch_size = 32\n","\n","# Get the training data: both images from CelebA and ImageNet\n","path_to_training_data = tf.keras.utils.get_file('train_face_2023_perturbed_small.h5', 'https://www.dropbox.com/s/tbra3danrk5x8h5/train_face_2023_perturbed_small.h5?dl=1')\n","# Instantiate a DatasetLoader using the downloaded dataset\n","train_loader = mdl.lab3.DatasetLoader(path_to_training_data, training=True, batch_size=batch_size)\n","test_loader = mdl.lab3.DatasetLoader(path_to_training_data, training=False, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"cREmhMWJ71EX"},"source":["### Building robustness to bias and uncertainty\n","\n","Remember that we'll be training our facial detection classifiers on the large, well-curated CelebA dataset (and ImageNet), and then evaluating their accuracy by testing them on an independent test dataset. We want to mitigate the effects of unwanted bias and uncertainty on the model's predictions and performance. Your goal is to build the best-performing, most robust model, one that achieves high classification accuracy across the entire test dataset.\n","\n","To achieve this, you may want to consider the three metrics introduced with Capsa: (1) representation bias, (2) data or aleatoric uncertainty, and (3) model or epistemic uncertainty. Note that all three of these metrics are different! For example, we can have well-represented examples that still have high epistemic uncertainty. Think about how you may use these metrics to improve the performance of your model."]},{"cell_type":"markdown","metadata":{"id":"1NhotGiT71EY"},"source":["# 2.2 Risk-aware facial detection with Capsa\n","\n","In the previous, we built a semi-supervised variational autoencoder (SS-VAE) to learn the latent structure of our database and to uncover feature representation disparities, inspired by the approach of [uncover hidden biases](http://introtodeeplearning.com/AAAI_MitigatingAlgorithmicBias.pdf). In this assignment, we'll show that we can use Capsa to build the same VAE in one line!\n","\n","This sets the foundation for quantifying a key risk metric -- representation bias -- for the facial detection problem. In working to improve your model's performance, you will want to consider representation bias carefully and think about how you could mitigate the effect of representation bias.\n","\n","Just like in the earlier assignment, we begin by defining a standard CNN-based classifier. We will then use Capsa to wrap the model and build the risk-aware VAE variant."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hQb75Vm71EZ"},"outputs":[],"source":["### Define the CNN classifier model ###\n","\n","'''Function to define a standard CNN model'''\n","def make_standard_classifier(n_outputs=1, n_filters=12):\n","  Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu')\n","  BatchNormalization = tf.keras.layers.BatchNormalization\n","  Flatten = tf.keras.layers.Flatten\n","  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n","\n","  model = tf.keras.Sequential([\n","    tf.keras.Input(shape=(64,64, 3)),\n","    Conv2D(filters=1*n_filters, kernel_size=5,  strides=2),\n","    BatchNormalization(),\n","\n","    Conv2D(filters=2*n_filters, kernel_size=5,  strides=2),\n","    BatchNormalization(),\n","\n","    Conv2D(filters=4*n_filters, kernel_size=3,  strides=2),\n","    BatchNormalization(),\n","\n","    Conv2D(filters=6*n_filters, kernel_size=3,  strides=2),\n","    BatchNormalization(),\n","\n","    Conv2D(filters=8*n_filters, kernel_size=3,  strides=2),\n","    BatchNormalization(),\n","\n","    Flatten(),\n","    Dense(512),\n","    Dense(n_outputs, activation=None),\n","  ])\n","  return model"]},{"cell_type":"markdown","metadata":{"id":"LgTG6buf71Ea"},"source":["### Capsa's `HistogramVAEWrapper`\n","\n","With our base classifier Capsa allows us to automatically define a VAE implementing that base classifier. Capsa's [`HistogramVAEWrapper`](https://themisai.io/capsa/api_documentation/HistogramVAEWrapper.html) builds this VAE to analyze the latent space distribution, just as we did in Lab 2.\n","\n","Specifically, `capsa.HistogramVAEWrapper` constructs a histogram with `num_bins` bins across every dimension of the latent space, and then calculates the joint probability of every sample according to the constructed histograms. The samples with the lowest joint probability have the lowest representation; the samples with the highest joint probability have the highest representation.\n","\n","`capsa.HistogramVAEWrapper` takes in a number of arguments including:\n","1. `base_model`: the model to be transformed into the risk-aware variant.\n","2. `num_bins`: the number of bins we want to discretize our distribution into.\n","2. `queue_size`: the number of samples we want to track at any given point.\n","3. `decoder`: the decoder architecture for the VAE.\n","\n","We define the same decoder as in Lab 2:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTat3K8E71Eb"},"outputs":[],"source":["### Define the decoder architecture for the facial detection VAE ###\n","\n","def make_face_decoder_network(n_filters=12):\n","  # Functionally define the different layer types we will use\n","  Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose,\n","                                      padding='same', activation='relu')\n","  BatchNormalization = tf.keras.layers.BatchNormalization\n","  Flatten = tf.keras.layers.Flatten\n","  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n","  Reshape = tf.keras.layers.Reshape\n","\n","  # Build the decoder network using the Sequential API\n","  decoder = tf.keras.Sequential([\n","    # Transform to pre-convolutional generation\n","    Dense(units=2*2*8*n_filters),  # 4x4 feature maps (with 6N occurances)\n","    Reshape(target_shape=(2, 2, 8*n_filters)),\n","\n","    # Upscaling convolutions (inverse of encoder)\n","    Conv2DTranspose(filters=6*n_filters, kernel_size=3,  strides=2),\n","    Conv2DTranspose(filters=4*n_filters, kernel_size=3,  strides=2),\n","    Conv2DTranspose(filters=2*n_filters, kernel_size=3,  strides=2),\n","    Conv2DTranspose(filters=1*n_filters, kernel_size=5,  strides=2),\n","    Conv2DTranspose(filters=3, kernel_size=5,  strides=2),\n","  ])\n","\n","  return decoder"]},{"cell_type":"markdown","metadata":{"id":"SzFGcrhv71Ed"},"source":["We are ready to create the wrapped model using `capsa.HistogramVAEWrapper` by passing in the relevant arguments!\n","\n","Just like in the wrappers in the Introduction to Capsa lab, we can take our standard CNN classifier, wrap it with `capsa.HistogramVAEWrapper`, build the wrapped model. The wrapper then enablings semi-supervised training for the facial detection task. As the wrapped model trains, the classifier weights are updated, and the VAE-wrapped model learns to track feature distributions over the latent space. More details of the `HistogramVAEWrapper` and how it can be used are [available here](https://themisai.io/capsa/api_documentation/HistogramVAEWrapper.html).\n","\n","We can then evaluate the representation bias of the classifier on the test dataset. By calling the `wrapped_model` on our test data, we can automatically generate representation bias and uncertainty scores that are normally manually calculated. Let's wrap our base CNN classifier using Capsa, train and build the resulting model, and start to process the test data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqsBHBf3yUlm"},"outputs":[],"source":["### Estimating representation bias with Capsa HistogramVAEWrapper ###\n","\n","model = make_standard_classifier()\n","# Wrap the CNN classifier for latent encoding with a VAE wrapper\n","wrapped_model = capsa.HistogramVAEWrapper(model, num_bins=5, queue_size=20000,\n","    latent_dim = 32, decoder=make_face_decoder_network())\n","\n","# Build the model for classification, defining the loss function, optimizer, and metrics\n","wrapped_model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n","    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), # for classification\n","    metrics=[tf.keras.metrics.BinaryAccuracy()], # for classification\n","    run_eagerly=True\n",")\n","\n","# Train the wrapped model for 6 epochs by fitting to the training data\n","history = wrapped_model.fit(\n","        train_loader,\n","        epochs=6,\n","        batch_size=batch_size,\n","  )\n","\n","## Evaluation\n","\n","# Get all faces from the testing dataset\n","test_imgs = test_loader.get_all_faces()\n","\n","# Call the Capsa-wrapped classifier to generate outputs: a RiskTensor dictionary consisting of predictions, uncertainty, and bias!\n","out = wrapped_model.predict(test_imgs, batch_size=512)\n"]},{"cell_type":"markdown","metadata":{"id":"629ng-_H6WOk"},"source":["# 2.3 Analyzing representation bias with Capsa\n","\n","From the above output, we have an estimate for the representation bias score! We can analyze the representation scores to start to think about manifestations of bias in the facial detection dataset. Before you run the next code block, which faces would you expect to be underrepresented in the dataset? Which ones do you think will be overrepresented?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OYMRqq5E71Ee"},"outputs":[],"source":["### Analyzing representation bias scores ###\n","\n","# Sort according to lowest to highest representation scores\n","indices = np.argsort(out.bias, axis=None) # sort the score values themselves\n","sorted_images = test_imgs[indices] # sort images from lowest to highest representations\n","sorted_biases = out.bias.numpy()[indices] # order the representation bias scores\n","sorted_preds = out.y_hat.numpy()[indices] # order the prediction values\n","\n","\n","# Visualize the 20 images with the lowest and highest representation in the test dataset\n","fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n","ax[0].imshow(mdl.util.create_grid_of_images(sorted_images[-20:], (4, 5)))\n","ax[0].set_title(\"Over-represented\")\n","\n","ax[1].imshow(mdl.util.create_grid_of_images(sorted_images[:20], (4, 5)))\n","ax[1].set_title(\"Under-represented\");"]},{"cell_type":"markdown","metadata":{"id":"-JYmGMJF71Ef"},"source":["We can also quantify how the representation density relates to the classification accuracy by plotting the two against each other:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfzlOhWi71Ef"},"outputs":[],"source":["# Plot the representation density vs. the accuracy\n","plt.xlabel(\"Density (Representation)\")\n","plt.ylabel(\"Accuracy\")\n","averaged_imgs = mdl.lab3.plot_accuracy_vs_risk(sorted_images, sorted_biases, sorted_preds, \"Bias vs. Accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"i8ERzg2-71Ef"},"source":["These representations scores relate back to data examples, so we can visualize what the average face looks like for a given *percentile* of representation density:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kn9IpPKYSECg"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(15,5))\n","ax.imshow(mdl.util.create_grid_of_images(averaged_imgs, (1,10)))"]},{"cell_type":"markdown","metadata":{"id":"cRNV-3SU71Eg"},"source":["#### **TODO: Scoring representation densities with Capsa**\n","\n","Write short answers to the questions below to complete the `TODO`s:\n","\n","1. How does accuracy relate to the representation score? From this relationship, what can you determine about the bias underlying the dataset?\n","2. What does the average face in the 10th percentile of representation density look like (i.e., the face for which 10% of the data have lower probability of occuring)? What about the 90th percentile? What changes across these faces?\n","3. What could be potential limitations of the `HistogramVAEWrapper` approach as it is implemented now?"]},{"cell_type":"markdown","metadata":{"id":"ww5lx7ue71Eg"},"source":["# 2.4 Analyzing epistemic uncertainty with Capsa\n","\n","Recall that *epistemic* uncertainty, or a model's uncertainty in its prediction, can arise from out-of-distribution data, missing data, or samples that are harder to learn. This does not necessarily correlate with representation bias! Imagine the scenario of training an object detector for self-driving cars: even if the model is presented with many cluttered scenes, these samples still may be harder to learn than scenes with very few objects in them.\n","\n","We will now use our VAE-wrapped facial detection classifier to analyze and estimate the epistemic uncertainty of the model trained on the facial detection task.\n","\n","While most methods of estimating epistemic uncertainty are *sampling-based*, we can also use ***reconstruction-based*** methods -- like using VAEs -- to estimate epistemic uncertainty. If a model is unable to provide a good reconstruction for a given data point, it has not learned that area of the underlying data distribution well, and therefore has high epistemic uncertainty.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NEfeWo2p7wKm"},"source":["Since we've already used the `HistogramVAEWrapper` to calculate the histograms for representation bias quantification, we can use the exact same VAE wrapper to shed insight into epistemic uncertainty! Capsa helps us do exactly that. When we called the model, we returned the classification prediction, uncertainty, and bias for every sample:\n","`predictions, uncertainty, bias = wrapped_model.predict(test_imgs, batch_size=512)`.\n","\n","Let's analyze these estimated uncertainties:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AwGPvdZm71Eg"},"outputs":[],"source":["### Analyzing epistemic uncertainty estimates ###\n","\n","# Sort according to epistemic uncertainty estimates\n","epistemic_indices = np.argsort(out.epistemic, axis=None) # sort the uncertainty values\n","epistemic_images = test_imgs[epistemic_indices] # sort images from lowest to highest uncertainty\n","sorted_epistemic = out.epistemic.numpy()[epistemic_indices] # order the uncertainty scores\n","sorted_epistemic_preds = out.y_hat.numpy()[epistemic_indices] # order the prediction values\n","\n","\n","# Visualize the 20 images with the LEAST and MOST epistemic uncertainty\n","fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n","ax[0].imshow(mdl.util.create_grid_of_images(epistemic_images[:20], (4, 5)))\n","ax[0].set_title(\"Least Uncertain\");\n","\n","ax[1].imshow(mdl.util.create_grid_of_images(epistemic_images[-20:], (4, 5)))\n","ax[1].set_title(\"Most Uncertain\");"]},{"cell_type":"markdown","metadata":{"id":"L0dA8EyX71Eh"},"source":["We quantify how the epistemic uncertainty relates to the classification accuracy by plotting the two against each other:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzQwvSvA71Eh"},"outputs":[],"source":["# Plot epistemic uncertainty vs. classification accuracy\n","plt.xlabel(\"Epistemic Uncertainty\")\n","plt.ylabel(\"Accuracy\")\n","_ = mdl.lab3.plot_accuracy_vs_risk(epistemic_images, sorted_epistemic, sorted_epistemic_preds, \"Epistemic Uncertainty vs. Accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"iyn0IE6x71Eh"},"source":["#### **TODO: Estimating epistemic uncertainties with Capsa**\n","\n","Write short answers to the questions below to complete the `TODO`s:\n","\n","1. How does accuracy relate to the epistemic uncertainty?\n","2. How do the results for epistemic uncertainty compare to the results for representation bias? Was this expected or unexpted? Why?\n","3. What may be instances in the facial detection task that could have high representation density but also high uncertainty?"]},{"cell_type":"markdown","metadata":{"id":"XbwRbesM71Eh"},"source":["# 2.5 Resampling based on risk metrics\n","\n","Finally, we will use the risk metrics just computed to actually *mitigate* the issues of bias and uncertainty in the facial detection classifier.\n","\n","Specifically, we will use the latent variables learned via the VAE to adaptively re-sample the face (CelebA) data during training, following the approach of [recent work](http://introtodeeplearning.com/AAAI_MitigatingAlgorithmicBias.pdf). We will alter the probability that a given image is used during training based on how often its latent features appear in the dataset. So, faces with rarer features (like dark skin, sunglasses, or hats) should become more likely to be sampled during training, while the sampling probability for faces with features that are over-represented in the training dataset should decrease (relative to uniform random sampling across the training data).\n","\n","Note that we want to debias and amplify only the *positive* samples in the dataset -- the faces -- so we are going to only adjust probabilities and calculate scores for these samples. We focus on using the representation bias scores to implement this adaptive resampling to achieve model debiasing.\n","\n","We re-define the wrapped model with `HistogramVAEWrapper`, and then define the adaptive resampling operation for training. At each training epoch, we compute the predictions, uncertainties, and representation bias scores, then recompute the data sampling probabilities according to the *inverse* of the representation bias score. That is, samples with higher representation densities will end up with lower re-sampling probabilities; samples with lower representations will end up with higher re-sampling probabilities.\n","\n","Let's do all this below!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q99jG0dB71Ei"},"outputs":[],"source":["### Define the standard CNN classifier and wrap with HistogramVAE ###\n","\n","classifier = make_standard_classifier()\n","# Wrap with HistogramVAE\n","wrapper = capsa.HistogramVAEWrapper(classifier, latent_dim=32, num_bins=5,\n","                          queue_size=2000, decoder=make_face_decoder_network())\n","\n","# Build the wrapped model for the classification task\n","wrapper.compile(optimizer=tf.keras.optimizers.Adam(5e-4),\n","                loss=tf.keras.losses.BinaryCrossentropy(),\n","                metrics=[tf.keras.metrics.BinaryAccuracy()])\n","\n","# Load training data\n","train_imgs = train_loader.get_all_faces()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"odFu4u7i71Ei"},"outputs":[],"source":["### Debiasing via resampling based on risk metrics ###\n","\n","# The training loop -- outer loop iterates over the number of epochs\n","num_epochs = 6\n","for i in range(num_epochs):\n","  print(\"Starting epoch {}/{}\".format(i+1, num_epochs))\n","\n","  # Get a batch of training data and compute the training step\n","  for step, data in enumerate(train_loader):\n","    metrics = wrapper.train_step(data)\n","    if step % 100 == 0:\n","        print(step)\n","\n","  # After the epoch is done, recompute data sampling proabilities\n","  #  according to the inverse of the bias\n","  out = wrapper(train_imgs)\n","\n","  # Increase the probability of sampling under-represented datapoints by setting\n","  #   the probability to the **inverse** of the biases\n","  inverse_bias = 1.0 / (np.mean(out.bias.numpy(),axis=-1) + 1e-7)\n","\n","  # Normalize the inverse biases in order to convert them to probabilities\n","  p_faces = inverse_bias / np.sum(inverse_bias)\n","\n","  # Update the training data loader to sample according to this new distribution\n","  train_loader.p_pos = p_faces"]},{"cell_type":"markdown","metadata":{"id":"SwXrAeBo71Ej"},"source":["That's it! We should have a debiased model (we hope!). Let's see how the model does.\n","\n","### Evaluation\n","\n","Let's run the same analyses as before, and plot the classification accuracy vs. the representation bias and classification accuracy vs. epistemic uncertainty. We want the model to do better across the data samples, achieving higher accuracies on the under-represented and more uncertain samples compared to previously.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yaMnwd_w71Ej"},"outputs":[],"source":["### Evaluation of debiased model ###\n","\n","# Get classification predictions, uncertainties, and representation bias scores\n","out = wrapper.predict(test_imgs)\n","\n","# Sort according to lowest to highest representation scores\n","indices = np.argsort(out.bias, axis=None)\n","bias_images = test_imgs[indices] # sort the images\n","sorted_bias = out.bias.numpy()[indices] # sort the representation bias scores\n","sorted_bias_preds = out.y_hat.numpy()[indices] # sort the predictions\n","\n","# Plot the representation bias vs. the accuracy\n","plt.xlabel(\"Density (Representation)\")\n","plt.ylabel(\"Accuracy\")\n","_ = mdl.lab3.plot_accuracy_vs_risk(bias_images, sorted_bias, sorted_bias_preds, \"Bias vs. Accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"d1cEEnII71Ej"},"source":["# 3.5 Conclusion!\n","\n","We encourage you to identify other questions that could be solved with Capsa and use those as the basis of your submission. But, to help get you started, here are some interesting questions that you might look into solving with these new tools and knowledge that you've built up:\n","\n","1. In this assignment, you learned how to build a wrapper that can estimate the bias within the training data, and take the results from this wrapper to adaptively re-sample during training to encourage learning on under-represented data.\n","  * Can we apply a similar approach to mitigate epistemic uncertainty in the model?\n","  * Can this approach be combined with your original bias mitigation approach to achieve robustness across both bias *and* uncertainty?\n","\n","2. In this assignment, you focused on the `HistogramVAEWrapper`.\n","  * How can you use other methods of uncertainty in Capsa to strengthen your uncertainty estimates? Checkout [Capsa documentation](https://themisai.io/capsa/api_documentation/index.html) for a list of all wrappers, and ask for help if you run into trouble applying them to your model!\n","  * Can you combine uncertainty estimates from different wrappers to achieve greater robustness in your estimates?\n","\n","3. So far in this part of the assignment, we focused only on bias and epistemic uncertainty. What about aleatoric uncetainty?\n","  * We've curated a dataset (available at [this URL](https://www.dropbox.com/s/wsdyma8a340k8lw/train_face_2023_perturbed_large.h5?dl=0)) of faces with greater amounts of aleatoric uncertainty -- can you use Capsa to wrap your model, estimate aleatoric uncertainty, and remove it from the dataset?\n","  * Does removing aleatoric uncertainty help improve your training accuracy on this new dataset?\n","  * Can you develop an approach to incorporate this aleatoric uncertainty estimation into the predictive training pipeline in order to improve accuracy? You may find some surprising results!!\n","\n","4. How can the performance of the classifier above be improved even further? We purposely did not optimize hyperparameters to leave this up to you!\n","\n","5. Are there other applications that you think Capsa and bias/uncertainty estimation would be helpful in?\n","  * Try integrating Capsa into another domain or dataset and submit your findings!\n","  * Are there applications where you may *not* want to debias your model?\n","\n","\n","We encourage you to think about and maybe even address some questions raised by this assignment and dig into any questions that you may have about the risks inherrent to neural networks and their data.\n","\n","<img src=\"https://i.ibb.co/BjLSRMM/ezgif-2-253dfd3f9097.gif\" />"]}],"metadata":{"colab":{"provenance":[{"file_id":"1IftmmgkpkRptNI5b4WttJWzEj9e80q26","timestamp":1735709741975}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}